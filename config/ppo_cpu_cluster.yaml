# PPO Configuration for Distributed CPU Cluster Training

# Environment settings
env:
  name: "procgen:procgen-coinrun-v0"  # Procgen environment name
  num_envs: 64  # Number of parallel environments per worker
  seed: 42  # Random seed (null for random)

# Training hyperparameters
training:
  total_timesteps: 100000000  # Total training timesteps (larger for cluster)
  n_steps: 128  # Steps per rollout (optimized for CPU)
  batch_size: 2048  # Batch size for training
  n_epochs: 4  # Number of epochs per update
  
# PPO hyperparameters
ppo:
  learning_rate: 0.0003  # Learning rate (3e-4)
  gamma: 0.999  # Discount factor
  gae_lambda: 0.95  # GAE lambda
  clip_range: 0.2  # PPO clipping parameter
  clip_range_vf: null  # Value function clipping (null for no clipping)
  normalize_advantage: true  # Normalize advantages
  ent_coef: 0.01  # Entropy coefficient
  vf_coef: 0.5  # Value function coefficient
  max_grad_norm: 0.5  # Maximum gradient norm

# Model architecture
model:
  hidden_size: 512  # Hidden layer size

# Logging and checkpointing
logging:
  log_interval: 1  # Log every N updates
  save_interval: 50  # Save checkpoint every N updates (more frequent for cluster)
  eval_episodes: 10  # Number of episodes for evaluation
  checkpoint_dir: "./checkpoints/ppo_cluster"
  log_dir: "./logs"

# Hardware - CPU Cluster Configuration
hardware:
  device: "cpu"  # Always CPU for cluster
  num_threads: null  # Auto-detect per node (or set specific value)
  
# Cluster settings (for future distributed implementation)
cluster:
  num_workers: 4  # Number of worker nodes
  distributed_backend: "gloo"  # Use gloo for CPU (not nccl)
  master_addr: "localhost"  # Master node address
  master_port: 29500  # Master node port

